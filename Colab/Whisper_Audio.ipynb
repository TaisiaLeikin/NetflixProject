{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaisiaLeikin/NetflixProject/blob/main/Colab/Whisper_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8glStTLseMc"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1FnLVIqEV1Tt5rCEOIk5OxihO6xdZgTMe\"><br>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">    נכתב על ידי <a href=\"mailto:odedzarchia@tauex.tau.ac.il\">עודד זרחיה</a> | <a href=\"https://github.com/Sourasky-DHLAB\">Github</a>\n",
        "       עריכת וידאו:  <a href=\"idoah@tauex.tau.ac.il\">עידו אהרון</a><br>\n",
        "    <a href=\"https://cenlib.tau.ac.il/\">הספרייה המרכזית ע\"ש סוראסקי</a> | <a href=\"https://tau.ac.il/\">אוניברסיטת תל אביב</a>\n",
        "</div></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G0FE9s4skOt"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h1><strong>1. תמלול קבצי אודיו באמצעות Whisper</strong></h1>\n",
        "<p>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "<a href=\"https://openai.com/blog/whisper/\"> Whisper </a>הוא מודל לזיהוי דיבור מבית <a href=\"https://openai.com/\">OpenAI</a> הזמין לציבור הרחב בקוד פתוח. מודל זה אומן על יותר מ-680 אלף שעות של שיחות באנגלית ובשפות רבות אחרות – בהן גם עברית וערבית.<br>\n",
        "מחברת זו תדריך אתכם בתמלול קטעי אודיו באמצעות Whisper. תוכלו להשתמש במחברת כפי שהיא כדי לאחסן את קבצי התמליל ב-Google Drive.<br>\n",
        "לתשומת לבכם - השימוש במחברת זו ובמודל של Whisper חופשי לחלוטין וללא שום עלות. בנוסף, אין הגבלה על אורך קטעי הוידאו/אודיו שניתן לתמלל באמצעות Whisper.</br> למחברת זו מצורף סרטון הדרכה קצר בשפה העברית. ליחצו על התמונה מתחת לתא זה כדי לצפות בו.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Getting Started Whisper](https://raw.githubusercontent.com/Sourasky-DHLAB/Whisper/main/Resources/video_prev.png)](https://youtu.be/cWON2CB-aRU)\n"
      ],
      "metadata": {
        "id": "Goa8S2-tbqCP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsyokosnuUdk"
      },
      "source": [
        "\n",
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>2. בדוק את סוג המעבד הגרפי (GPU) </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">סוג <a href=\"https://https://he.wikipedia.org/wiki/%D7%9E%D7%A2%D7%91%D7%93_%D7%92%D7%A8%D7%A4%D7%99\">המעבד הגרפי</a> <strong>(GPU - Graphics Processing Unit)</strong> שאתם מקבלים ב-Google Colab מגדיר את המהירות שבה קטעי האודיו יתומללו. ככל שמספר <a href=\"https://https://he.wikipedia.org/wiki/FLOPS\">הפלופס</a> <strong>(FLOPS - Floating Point Operations Per Second, פעולות נקודות צפות לשנייה)</strong> גבוה יותר, כך התמלול מהיר יותר. יחד עם זאת, גם המעבד הגרפי החלש ביותר ב-Colab מסוגל להריץ כל מודל של Whisper. יש לוודא כי בחרתם ב-GPU כמאיץ חומרה עבור מחברת זו (Runtime → Change runtime type → Hardware accelerator).</p>\n",
        "</div>\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>GPU</th>\n",
        "      <th>GPU RAM</th>\n",
        "      <th>FP32 teraFLOPS</th>\n",
        "      <th>Availability</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>T4</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>8.1</td>\n",
        "      <td>Free</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>P100</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>10.6</td>\n",
        "      <td>Colab Pro</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>V100</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>15.7</td>\n",
        "      <td>Colab Pro (Rare)</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לוודא את סוג המעבד הגרפי שהוקצה עבור מחברת זו. אפסו את ה-runtime של המחברת אם ברצונכם לקבל מעבד גרפי אחר (Runtime → Restart runtime).</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rr1Qg5MwuYEG",
        "outputId": "166b90ce-5dca-45dd-aa7c-900b98b3428c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-5972a417-4f67-165f-456e-8fe5ce405bbe)\n",
            "Tue Jan 28 10:08:27 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check gpu type\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAo5l6xufyF"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>3. התקנת ספריות</strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להוריד ולהתקין את ספריית <a href=\"https://github.com/openai/whisper\">Whisper</a> הנחוצה לפעולת התמלול.</p></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vf4cqkCaZn0U",
        "outputId": "ffc1aa1e-8ccb-4be7-ecc5-1d3f617d90ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-kt9g0boi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-kt9g0boi\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20240930) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=9bc42dcd4a3e60f7650efee867994010c8bbdce7417e525f924188bc965b83e8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9lgcytex/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.5 rapidfuzz-3.11.0\n"
          ]
        }
      ],
      "source": [
        "# install libraries\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXHJxr7ru7Xr"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <h1><strong>4. ייבוא ספריות </strong></h1>\n",
        "  <div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לייבא את הספריות הנדרשות עבור פעולת התמלול.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rC-mpBTgZr3D"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import whisper\n",
        "from whisper.utils import get_writer\n",
        "import os\n",
        "import string\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4K42pokvEGa"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>5. חיבור ל-Google Drive </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להתחבר ל-Google Drive האישי שלכם. כדי לראות את הקבצים שלכם פתחו את סייר הקבצים בתפריט השמאלי.</p></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9LpaqE6ePPIK",
        "outputId": "aa9d45e5-dbd8-493c-898d-d33db3cce631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) # This will prompt for authorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDtp_AGUvPiS"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>6. הגדרת תיקיות </strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להגדיר את מיקום קבצי האודיו והתמליל:<br>\n",
        "    <ol style=\"float:right;\">\n",
        "      <li>קבצי אודיו - מחברת זו מניחה כי קבצי האודיו לתמלול נמצאים ב-Google Drive תחת תיקיית <code>Whisper/Audio/</code>. ניתן לשנות הגדרה זו במשתנה <code>audio_folder_path</code>.</li>\n",
        "      <li>קבצי תמליל - מחברת זו מניחה כי קבצי הטקסט של התמלול יישמרו ב-Google Drive תחת תיקיית <code>Whisper/Transcriptions/</code>. ניתן לשנות הגדרה זו במשתנה <code>transcription_folder_path</code>. במידה ואינה קיימת, תיקייה זו תיווצר באופן אוטומטי לאחר הרצת התא.</li>\n",
        "    </ol>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e_fKqCK9OQLE"
      },
      "outputs": [],
      "source": [
        "# Assuming the audio files are in a folder called \"Audio\" under \"Whisper\"\n",
        "audio_folder_path = \"/content/drive/MyDrive/Whisper/Audio/\"\n",
        "\n",
        "# Assuming the text files will be placed in a folder called \"Transcriptions\" under \"Whisper\"\n",
        "transcription_folder_path = \"/content/drive/MyDrive/Whisper/Transcriptions/\"\n",
        "\n",
        "# Create \"Transcriptions\" folder if does not exist\n",
        "if not os.path.exists(transcription_folder_path):\n",
        "  os.makedirs(transcription_folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyZ4xlequt4r"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>7. טעינת קבצי אודיו לתמלול </div></h1></strong>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לטעון את קבצי האודיו הנמצאים בתיקיית <code>Whisper/Audio/</code> לתוך משתנה מסוג <a href=\"https://he.wikipedia.org/wiki/%D7%A8%D7%A9%D7%99%D7%9E%D7%94_(%D7%9E%D7%91%D7%A0%D7%94_%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D)\">רשימה</a> (list). הפלט יציג את שמות וכמות הקבצים לתמלול.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lr0_KI8N0mWY",
        "outputId": "3306e1ff-69c7-422d-f61e-accabac05624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Whisper/Audio/audio1773056431.m4a\n",
            "\u001b[1m There are 1 audio files to transcribe\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all the file paths in the folder\n",
        "audio_files = []\n",
        "for file in os.listdir(audio_folder_path):\n",
        "    audio_files.append(audio_folder_path + file)\n",
        "for p in audio_files:\n",
        "    print(p)\n",
        "print(f\"\\033[1m There are {len(audio_files)} audio files to transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDOniT4Y61aH"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>8. טעינת מודל התמלול </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לטעון את מודל התמלול המתאים. יושם לב כי אנו משתמשים במודל הגדול (<strong>large</strong>), המדויק ביותר עבור השפה העברית. <br> ישנם חמישה מודלים, ארבעה עם גרסאות באנגלית בלבד, המציעים פשרות שונות בין מהירות לדיוק. יושם לב כי פעולת התמלול צורכת משאבי מחשוב מרובים. לפיכך, אם ברצונכם להשתמש במודל הגדול עבור קבצים מרובים, מומלץ להריץ את המחברת בסביבה עם משאבי מחשוב מובטחים כדוגמת <a href=\"https://colab.research.google.com/signup\">Google Colab Pro</a>. עבור תמלול באנגלית בלבד, מודלי ה-en. נוטים לתפקד טוב יותר. <br>בטבלה שלהלן מוצגים שמות המודלים הזמינים יחד עם דרישות הזיכרון והמהירות היחסית שלהם:\n",
        "</div>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"text-align:center\">Size</th>\n",
        "<th style=\"text-align:center\">Parameters</th>\n",
        "<th style=\"text-align:center\">English-only model</th>\n",
        "<th style=\"text-align:center\">Multilingual model</th>\n",
        "<th style=\"text-align:center\">Required VRAM</th>\n",
        "<th style=\"text-align:center\">Relative speed</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">tiny</td>\n",
        "<td style=\"text-align:center\">39 M</td>\n",
        "<td style=\"text-align:center\"><code>tiny.en</code></td>\n",
        "<td style=\"text-align:center\"><code>tiny</code></td>\n",
        "<td style=\"text-align:center\">~1 GB</td>\n",
        "<td style=\"text-align:center\">~10x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">base</td>\n",
        "<td style=\"text-align:center\">74 M</td>\n",
        "<td style=\"text-align:center\"><code>base.en</code></td>\n",
        "<td style=\"text-align:center\"><code>base</code></td>\n",
        "<td style=\"text-align:center\">~1 GB</td>\n",
        "<td style=\"text-align:center\">~7x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">small</td>\n",
        "<td style=\"text-align:center\">244 M</td>\n",
        "<td style=\"text-align:center\"><code>small.en</code></td>\n",
        "<td style=\"text-align:center\"><code>small</code></td>\n",
        "<td style=\"text-align:center\">~2 GB</td>\n",
        "<td style=\"text-align:center\">~4x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">medium</td>\n",
        "<td style=\"text-align:center\">769 M</td>\n",
        "<td style=\"text-align:center\"><code>medium.en</code></td>\n",
        "<td style=\"text-align:center\"><code>medium</code></td>\n",
        "<td style=\"text-align:center\">~5 GB</td>\n",
        "<td style=\"text-align:center\">~2x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">large</td>\n",
        "<td style=\"text-align:center\">1550 M</td>\n",
        "<td style=\"text-align:center\">N/A</td>\n",
        "<td style=\"text-align:center\"><code>large</code></td>\n",
        "<td style=\"text-align:center\">~10 GB</td>\n",
        "<td style=\"text-align:center\">~1x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">turbo</td>\n",
        "<td style=\"text-align:center\">809 M</td>\n",
        "<td style=\"text-align:center\">N/A</td>\n",
        "<td style=\"text-align:center\"><code>turbo</code></td>\n",
        "<td style=\"text-align:center\">~6 GB</td>\n",
        "<td style=\"text-align:center\">~8x</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<p><div style=\"direction:rtl\" dir=\"rtl\"><br></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ToWWQiTv7foY",
        "outputId": "62059507-ef8c-4c37-d111-94ca50b498e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:45<00:00, 68.0MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# load whisper model\n",
        "model = whisper.load_model(\"large\")\n",
        "print(f\"\\033[1m Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D_Xnwk0_ZIG"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>9. קביעת שפה לתמלול וסוג הפלט</div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        " <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לקבוע שני משתנים:\n",
        "<ol style=\"float:right;\">\n",
        "  <li><code>lang</code> - הגדרת שפת התמלול. <code>he</code> משמעו עברית. ניתן להדפיס את רשימת השפות הנתמכת על ידי הרצת הפקודה <code>print(whisper.tokenizer.LANGUAGES)</code> בתא נפרד.</li>\n",
        "  <li><code>output_format</code> - הגדרת סוג הפלט. Whisper תומך בהפקת הפלטים הבאים:\n",
        "    <ul>\n",
        "      <li><code>txt</code> - קובץ טקסט עם מעברי שורות, ניתן לפתיחה בכל מעבד תמלילים.</li>\n",
        "      <li><code>srt/vtt</code>- קבצי כתוביות המכילים טקסט וחתימות זמן.</li>\n",
        "      <li><code>tsv</code>- קובץ טקסט מופרד בטאבים עם חלוקה לסגמנטים. ניתן לפתיחה כגיליון אלקטרוני.</li>\n",
        "      <li><code>json</code>- מבנה מידע המורכב מזוגות של מפתח-ערך.</li>\n",
        "    </ul><br>\n",
        "    החליפו את ערך המשתנה בהתאם לסוג הפלט שברצונכם לקבל. ניתן להפיק את כל סוגי הפלטים באמצעות קביעת הערך <code>all</code>.</p>\n",
        "  </li>\n",
        "</ol>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZG7xbllv_xvO"
      },
      "outputs": [],
      "source": [
        "lang = 'he'\n",
        "output_format = 'txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elbd5zmB-_1a"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>10. תמלול </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לתמלל את קבצי האודיו בתיקיית <code>Whisper/Audio</code>.\n",
        "קבצי הטקסט יישמרו בתיקיית <code>Whisper/Transcriptions/</code> עם שם זהה לקבצי האודיו אך עם סיומת txt. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "adgPhhduOSyz",
        "outputId": "f418c154-4d48-4803-aa85-e4fbd1466727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 188966/188966 [07:28<00:00, 420.96frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Whisper/Audio/audio1773056431.m4a\n",
            "\u001b[1m--- Transcribed 1 audio files in 457.9965772628784 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# set timer\n",
        "import time\n",
        "start_time = time.time()\n",
        "# transcribe audio files in list\n",
        "for p in audio_files:\n",
        "  result = model.transcribe(p, verbose = False, language = lang) # to translate add task = 'transalte'\n",
        "  options = {\n",
        "        \"max_line_width\": None,\n",
        "        \"max_line_count\": None,\n",
        "        \"highlight_words\": False\n",
        "    }\n",
        "  # use get_writer method to output files\n",
        "  output_file = get_writer(output_format, transcription_folder_path)\n",
        "  output_file(result, p[:-4], options=options)\n",
        "  print(p)\n",
        "print(f\"\\033[1m--- Transcribed {len(audio_files)} audio files in %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNRHYaXRutE"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>11. בדיקת איכות (אופציונלי) </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "\n",
        "<p style=\"text-align: right; direction: rtl; float: right;\">הרצת התאים הבאים תאפשר לכם להעריך את הדיוק של התמלול שהפקתם באמצעות Whisper. בדיקה זו מתבצעת במספר שלבים, ומצריכה את קובץ התמלול המקורי (<a href=\"https://https://en.wikipedia.org/wiki/Ground_truth\">Ground Truth</a>): </p>\n",
        "<ol>\n",
        "<li>נירמול פלט התמלול של Whisper וקובץ התמלול המקורי  - הסרת סימני פיסוק, רווחים מיותרים, קפיטליזציה וכו&#39;. הנתיב של הקובץ שהפקתם באמצעות Whisper מוגדר במשתנה מוגדר במשתנה <code>whisper_output</code>. הנתיב של קובץ התמלול המקורי מוגדר במשתנה <code>ground_truth</code>.</li>\n",
        "<li>השוואה בין שני הקבצים וחישוב אחוז השגיאות בהתאם למדד <a href=\"https://https://en.wikipedia.org/wiki/Word_error_rate\">Word Error Rate</a> - באמצעות ספריית jiwer.</li>\n",
        "</ol>\n",
        "<p style=\"text-align: right; direction: rtl; float: right;\">הפלט המתקבל מוצג באחוזים ומצביע על אחוז המילים השגויות בקובץ התמלול, וזאת בהשוואה לקובץ המקור. כלומר, במידה ואחוז השגיאות עומד על כ-3%, אזי מתוך 100 מילים יש 3 מילים שגויות.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL1NPgEhSONs"
      },
      "outputs": [],
      "source": [
        "# normalize whisper output\n",
        "whisper_output = '/content/drive/MyDrive/Whisper/Transcriptions/Copy of Snyder.txt'\n",
        "whisper_output_norm = open(whisper_output, 'r').read().lower().translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "print(whisper_output_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTOY_r7kUsBc"
      },
      "outputs": [],
      "source": [
        "# normalize ground truth\n",
        "ground_truth = '/content/drive/MyDrive/Whisper/GT/Snyder_GT.txt'\n",
        "ground_truth_norm = open(ground_truth, 'r').read().lower().translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "print(ground_truth_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6mR6ErlVUwD"
      },
      "outputs": [],
      "source": [
        "# calculate WER\n",
        "reference = ground_truth_norm\n",
        "hypothesis = whisper_output_norm\n",
        "error = wer(reference, hypothesis)\n",
        "error_percentage = \"{:.0%}\".format(error)\n",
        "print(error_percentage)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}